{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"name": "ML_with_PyTorch_chapter.ipynb", "provenance": [], "collapsed_sections": []}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "accelerator": "GPU", "widgets": {"application/vnd.jupyter.widget-state+json": {"35b792c776b14c4f95397a4c0582724c": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_view_name": "HBoxView", "_dom_classes": [], "_model_name": "HBoxModel", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.5.0", "box_style": "", "layout": "IPY_MODEL_07161f98f15e41cba988e5dc3e3bdc6f", "_model_module": "@jupyter-widgets/controls", "children": ["IPY_MODEL_77773fe182f7493bafa9f805c892f678", "IPY_MODEL_b42d18a69e8647578ffaed0dcd941f6b"]}}, "07161f98f15e41cba988e5dc3e3bdc6f": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "77773fe182f7493bafa9f805c892f678": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_view_name": "ProgressView", "style": "IPY_MODEL_f7970cef164d4d08922fbfe03b7c350d", "_dom_classes": [], "description": "", "_model_name": "FloatProgressModel", "bar_style": "success", "max": 1, "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": 1, "_view_count": null, "_view_module_version": "1.5.0", "orientation": "horizontal", "min": 0, "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_bfb02f5d05ff466bab62a75af13b905d"}}, "b42d18a69e8647578ffaed0dcd941f6b": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_view_name": "HTMLView", "style": "IPY_MODEL_4f6d2092f250451ab76520c830389dbb", "_dom_classes": [], "description": "", "_model_name": "HTMLModel", "placeholder": "\u200b", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": " 9920512/? [00:02&lt;00:00, 4054501.22it/s]", "_view_count": null, "_view_module_version": "1.5.0", "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_7b7f5f05cf4f45c498bda81ae08f82ee"}}, "f7970cef164d4d08922fbfe03b7c350d": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_view_name": "StyleView", "_model_name": "ProgressStyleModel", "description_width": "initial", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "bar_color": null, "_model_module": "@jupyter-widgets/controls"}}, "bfb02f5d05ff466bab62a75af13b905d": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "4f6d2092f250451ab76520c830389dbb": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_view_name": "StyleView", "_model_name": "DescriptionStyleModel", "description_width": "", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "_model_module": "@jupyter-widgets/controls"}}, "7b7f5f05cf4f45c498bda81ae08f82ee": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "117a06c408a3471aa9617a40e9fe7c67": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_view_name": "HBoxView", "_dom_classes": [], "_model_name": "HBoxModel", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.5.0", "box_style": "", "layout": "IPY_MODEL_f5def618a30c4c2c88d31afadc3e075e", "_model_module": "@jupyter-widgets/controls", "children": ["IPY_MODEL_439ad68a471243b09a1a45af9040611c", "IPY_MODEL_a8dbeb0c7dfa4119961239dddeb1078c"]}}, "f5def618a30c4c2c88d31afadc3e075e": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "439ad68a471243b09a1a45af9040611c": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_view_name": "ProgressView", "style": "IPY_MODEL_907ab7af35de4f348fb4f10b3fa9e2de", "_dom_classes": [], "description": "", "_model_name": "FloatProgressModel", "bar_style": "success", "max": 1, "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": 1, "_view_count": null, "_view_module_version": "1.5.0", "orientation": "horizontal", "min": 0, "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_03c8d306ea6547a58d946a7e28b80b0a"}}, "a8dbeb0c7dfa4119961239dddeb1078c": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_view_name": "HTMLView", "style": "IPY_MODEL_e31c76cce6b04876b9b22f6704c79eee", "_dom_classes": [], "description": "", "_model_name": "HTMLModel", "placeholder": "\u200b", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": " 32768/? [00:00&lt;00:00, 128686.71it/s]", "_view_count": null, "_view_module_version": "1.5.0", "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_027767c4a3ca4d6faaa45e7d03774fca"}}, "907ab7af35de4f348fb4f10b3fa9e2de": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_view_name": "StyleView", "_model_name": "ProgressStyleModel", "description_width": "initial", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "bar_color": null, "_model_module": "@jupyter-widgets/controls"}}, "03c8d306ea6547a58d946a7e28b80b0a": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "e31c76cce6b04876b9b22f6704c79eee": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_view_name": "StyleView", "_model_name": "DescriptionStyleModel", "description_width": "", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "_model_module": "@jupyter-widgets/controls"}}, "027767c4a3ca4d6faaa45e7d03774fca": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "6f5d75a1815b43a0805befaa7bf42d1a": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_view_name": "HBoxView", "_dom_classes": [], "_model_name": "HBoxModel", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.5.0", "box_style": "", "layout": "IPY_MODEL_e635aed602794e71aaa23d2c0693edd9", "_model_module": "@jupyter-widgets/controls", "children": ["IPY_MODEL_b3b4f3ff01674afa9269ec522ad77a8d", "IPY_MODEL_d83bf18dfa3548db8d82d4be145bbfe6"]}}, "e635aed602794e71aaa23d2c0693edd9": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "b3b4f3ff01674afa9269ec522ad77a8d": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_view_name": "ProgressView", "style": "IPY_MODEL_a4d7a3c499324f0db3e9e29960ff7f13", "_dom_classes": [], "description": "", "_model_name": "FloatProgressModel", "bar_style": "success", "max": 1, "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": 1, "_view_count": null, "_view_module_version": "1.5.0", "orientation": "horizontal", "min": 0, "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_774eabc0520d48b39a651b798a76415c"}}, "d83bf18dfa3548db8d82d4be145bbfe6": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_view_name": "HTMLView", "style": "IPY_MODEL_ad822186706a42e5999ee76584df97bb", "_dom_classes": [], "description": "", "_model_name": "HTMLModel", "placeholder": "\u200b", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": " 1654784/? [00:01&lt;00:00, 1479199.46it/s]", "_view_count": null, "_view_module_version": "1.5.0", "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_538841128f8c4ce28b39e65242b43267"}}, "a4d7a3c499324f0db3e9e29960ff7f13": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_view_name": "StyleView", "_model_name": "ProgressStyleModel", "description_width": "initial", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "bar_color": null, "_model_module": "@jupyter-widgets/controls"}}, "774eabc0520d48b39a651b798a76415c": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "ad822186706a42e5999ee76584df97bb": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_view_name": "StyleView", "_model_name": "DescriptionStyleModel", "description_width": "", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "_model_module": "@jupyter-widgets/controls"}}, "538841128f8c4ce28b39e65242b43267": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "9d200d8eb4f54c6e94d85dec832b5ca9": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_view_name": "HBoxView", "_dom_classes": [], "_model_name": "HBoxModel", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.5.0", "box_style": "", "layout": "IPY_MODEL_04ffe705e0784eecb1e926a387786032", "_model_module": "@jupyter-widgets/controls", "children": ["IPY_MODEL_9c8d0f74b6664f618a5719f0500f6a3d", "IPY_MODEL_afe3805a2f6944e4b8d45d2bf462485a"]}}, "04ffe705e0784eecb1e926a387786032": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "9c8d0f74b6664f618a5719f0500f6a3d": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_view_name": "ProgressView", "style": "IPY_MODEL_44f99c586cda4d689813e346921bd419", "_dom_classes": [], "description": "", "_model_name": "FloatProgressModel", "bar_style": "success", "max": 1, "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": 1, "_view_count": null, "_view_module_version": "1.5.0", "orientation": "horizontal", "min": 0, "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_915eb201b2d54cd3b6e1cde6c02a3861"}}, "afe3805a2f6944e4b8d45d2bf462485a": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_view_name": "HTMLView", "style": "IPY_MODEL_af563cf1cb414f96ae6b6b98502d945f", "_dom_classes": [], "description": "", "_model_name": "HTMLModel", "placeholder": "\u200b", "_view_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "value": " 8192/? [00:00&lt;00:00, 20610.56it/s]", "_view_count": null, "_view_module_version": "1.5.0", "description_tooltip": null, "_model_module": "@jupyter-widgets/controls", "layout": "IPY_MODEL_22330057c7a542708cf4f14bc6614269"}}, "44f99c586cda4d689813e346921bd419": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "model_module_version": "1.5.0", "state": {"_view_name": "StyleView", "_model_name": "ProgressStyleModel", "description_width": "initial", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "bar_color": null, "_model_module": "@jupyter-widgets/controls"}}, "915eb201b2d54cd3b6e1cde6c02a3861": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}, "af563cf1cb414f96ae6b6b98502d945f": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "model_module_version": "1.5.0", "state": {"_view_name": "StyleView", "_model_name": "DescriptionStyleModel", "description_width": "", "_view_module": "@jupyter-widgets/base", "_model_module_version": "1.5.0", "_view_count": null, "_view_module_version": "1.2.0", "_model_module": "@jupyter-widgets/controls"}}, "22330057c7a542708cf4f14bc6614269": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_view_name": "LayoutView", "grid_template_rows": null, "right": null, "justify_content": null, "_view_module": "@jupyter-widgets/base", "overflow": null, "_model_module_version": "1.2.0", "_view_count": null, "flex_flow": null, "width": null, "min_width": null, "border": null, "align_items": null, "bottom": null, "_model_module": "@jupyter-widgets/base", "top": null, "grid_column": null, "overflow_y": null, "overflow_x": null, "grid_auto_flow": null, "grid_area": null, "grid_template_columns": null, "flex": null, "_model_name": "LayoutModel", "justify_items": null, "grid_row": null, "max_height": null, "align_content": null, "visibility": null, "align_self": null, "height": null, "min_height": null, "padding": null, "grid_auto_rows": null, "grid_gap": null, "max_width": null, "order": null, "_view_module_version": "1.2.0", "grid_template_areas": null, "object_position": null, "object_fit": null, "grid_auto_columns": null, "margin": null, "display": null, "left": null}}}}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "FAcvCkLlsEgD"}, "source": ["# ML with PyTorch\n", "\n", "Copyright Luca de Alfaro, 2019-20. \n", "License: [CC-BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/4.0/)."]}, {"cell_type": "markdown", "metadata": {"id": "680c65eb5337eb22", "colab_type": "text"}, "source": "\nPrepared on: Thu Oct 28 11:35:53 2021\n\nThis is a book chapter; it is not a homework assignment.  \nDo not submit it as a solution to a homework assignment; you would receive no credit.\n"}, {"cell_type": "markdown", "metadata": {"id": "u9CZetyw5YiQ"}, "source": ["In the previous chapters, we have implemented a machine-learning framework from scratch, on the basis of a symbolic representation for expressions, and automated gradient computation.  It is now time for us to experiment with a serious machine-learning framework: we will tackle the problem of image recognition using [PyTorch](https://pytorch.org).  We will see that PyTorch works in a way that is fundamentally similar to our homegrown framework.  On the other hand, PyTorch is vastly more sophisticated in a number of aspects, including: \n", "\n", "* _Tensors._ Our variables represented one floating point number.  Many machine-learning problems have as input vectors or matrices of numbers; for example, an image is commonly modeled as a 3D-matrix of floating point numbers with dimensions $w \\times h \\times c$, where $w$ and $h$ are the image width and height, and where $c$ is the number of color channels ($c=1$ for black and white images, and $c=3$ for RGB color images).  PyTorch's unit of processing consists in [tensors](https://pytorch.org/docs/stable/tensors.html), which represent such multi-dimensional matrices of numbers. \n", "\n", "* _Layers and functions._  PyTorch contains [pre-defined layers and functions](https://pytorch.org/docs/stable/nn.html) (corresponding to the expressions of our framework) that many basic and useful machine-learning building blocks, such as fully-connected neural-network layers, convolutional neural-network layers, recurrent networks, and more. \n", "\n", "* _GPU optimizations._  PyTorch can exploit [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit) that support the [CUDA](https://en.wikipedia.org/wiki/CUDA) processing language; training or using neural networks on GPUs is often about a hundred times faster than on regular GPUs.  Platforms such as [Google Colab](https://colab.research.google.com) provide runtimes with GPUs. \n", "\n", "* _Batches._ Our framework processed input one point at a time.  PyTorch can process _batches_ of data, so that it can train on hundreds of images in the same step.  The use of batches makes machine learning more efficient, as well as better behaved.  We will discuss later how the batch size is chosen.\n", "\n", "* _Algorithms for tuning the step size._  Finding a rough step size is always a bit of guesswork, and PyTorch provides [many useful algorithms](https://pytorch.org/docs/stable/optim.html) to help tune the learning step size. "]}, {"cell_type": "markdown", "metadata": {"id": "xAmSe3BD9Xuo"}, "source": ["### [PyTorch](https://pytorch.org), TensorFlow, and scikit-learn\n", "\n", "PyTorch and [TensorFlow](https://www.tensorflow.org/) are likely the two most commonly used deep-learning machine learning frameworks available today.  Another widely used machine-learning framework is [scikit-learn](https://scikit-learn.org/stable/), pronounced similarly to _psychic learn_. \n", "\n", "scikit-learn provides a greater variety of learning algorithms than PyTorch or TensorFlow, including decision trees, SVMs, and more.  On the other hand, scikit-learn does not provide the same optimizations as PyTorch and TensorFlow to work efficiently on vast amounts of data, and neither does scikit-learn provide support for GPU acceleration. \n", "\n", "PyTorch and TensorFlow are much closer to each other, and often the decision of using one of them rather than the other stems mainly from personal preference.  The main reason why we chose PyTorch for this chapter is that its gradient-computation methods are very close to the ones we developed for our homebrew framework."]}, {"cell_type": "markdown", "metadata": {"id": "8YhgY81mKUeb"}, "source": ["## A flat neural network for MNIST\n", "\n", "The first problem we will tackle will be the one of _digit recognition_.  We will use a dataset made of single digits, each drawn on a $28 \\times 28$ canvas.  This dataset, called the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) [dataset](http://yann.lecun.com/exdb/mnist/), and the dataset has become the \"hello world\" of deep machine learning.  To use it, we first load PyTorch.  If using Google Colab, make sure you have selected a GPU backend. "]}, {"cell_type": "code", "metadata": {"id": "c484AFnQxgEZ"}, "source": ["import numpy as np\n", "import matplotlib\n", "import matplotlib.pyplot as plt\n", "import random\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "from torchvision import datasets, transforms\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "zuZ3_Cez02U8"}, "source": ["# We use the CUDA device to get GPU acceleration.\n", "USE_CUDA = True\n", "if USE_CUDA:\n", "    device = torch.device(\"cuda\")\n", "    kwargs = {'num_workers': 1, 'pin_memory': True}\n", "else:\n", "    device = torch.device(\"cpu\")\n", "    kwargs = {}\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "3tyklQHIOoUC"}, "source": ["Let us load the dataset itself, and display its first digit. "]}, {"cell_type": "code", "metadata": {"id": "i0sZBlV0NH_r"}, "source": ["mnist = datasets.MNIST('../data', train=True, download=True,\n", "                          transform=transforms.ToTensor())\n", "digit_idx = 0\n", "print(\"This digit is a:\", mnist[digit_idx][1])\n", "plt.imshow(mnist[digit_idx][0].reshape(28,28), vmin=0., vmax=1., cmap='binary')\n"], "execution_count": null, "outputs": [{"output_type": "stream", "text": ["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"], "name": "stdout"}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "35b792c776b14c4f95397a4c0582724c", "version_minor": 0, "version_major": 2}, "text/plain": ["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]}, "metadata": {"tags": []}}, {"output_type": "stream", "text": ["Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n", "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"], "name": "stdout"}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "117a06c408a3471aa9617a40e9fe7c67", "version_minor": 0, "version_major": 2}, "text/plain": ["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]}, "metadata": {"tags": []}}, {"output_type": "stream", "text": ["Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n", "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n", "\n"], "name": "stdout"}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "6f5d75a1815b43a0805befaa7bf42d1a", "version_minor": 0, "version_major": 2}, "text/plain": ["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]}, "metadata": {"tags": []}}, {"output_type": "stream", "text": ["Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n", "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"], "name": "stdout"}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "9d200d8eb4f54c6e94d85dec832b5ca9", "version_minor": 0, "version_major": 2}, "text/plain": ["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]}, "metadata": {"tags": []}}, {"output_type": "stream", "text": ["Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n", "Processing...\n", "Done!\n", "This digit is a: 5\n", "\n"], "name": "stdout"}, {"output_type": "stream", "text": ["/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n", "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"], "name": "stderr"}, {"output_type": "execute_result", "data": {"text/plain": ["<matplotlib.image.AxesImage at 0x7fa4a3f64dd8>"]}, "metadata": {"tags": []}, "execution_count": 3}, {"output_type": "display_data", "data": {"image/png": "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\n", "text/plain": ["<Figure size 432x288 with 1 Axes>"]}, "metadata": {"tags": [], "needs_background": "light"}}, {"output_type": "stream", "text": ["\n", "\n"], "name": "stdout"}]}, {"cell_type": "markdown", "metadata": {"id": "3XOL4GOO0l5v"}, "source": ["### Defining the neural network\n", "\n", "We will be using a [fully connected neural net](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html), consisting of three layers.  The first layer has $28 \\times 28$ inputs, one for each pixel of the image, and `layer1_size` outputs (and so, `layer1_size` neurons); the second layer has `layer1_size` inputs and `layer2_size` outputs (and thus, neurons), and the final layer has `layer2_size` inputs and 10 outputs, one output for each of the 10 digit classes. \n", "\n", "The first two layers of the net consist of [ReLU units](https://en.wikipedia.org/wiki/Rectifier_(neural_networks).  A ReLU unit with $n$ inputs is defined by $n+1$ weights $b, w_1, \\ldots, w_n$, and when it is fed inputs $x_1, \\ldots, x_n$, it produces output \n", "\n", "$$\n", "\\max \\left( 0, b + \\sum_{i=1}^n w_i x_i \\right) \\; .\n", "$$\n", "\n", "The weight $b$ is called the _bias_ of the unit. \n", "We will be using not one neuron at a time, but one whole layer of neurons.  If the layer contains $m$ neurons, and we write $y_j$ for its $j$-th output, with $1 \\leq j \\leq m$, we have: \n", "\n", "$$\n", "y_j = \\max \\left( 0, b_j + \\sum_{i=1}^n w_{ji} x_i \\right) \\; ,\n", "$$\n", "\n", "or denoting by $b = [b_j]_{1 \\leq j \\leq m}$ the bias vector, and by $W = [w_{ji}]_{1 \\leq j \\leq m, 1 \\leq i \\leq n}$ the weight matrix, by: \n", "\n", "$$\n", "y = \\max \\left( 0, b + Wx \\right) \\; ,\n", "$$\n", "\n", "where $x$ is the input vector $x = [x_1, \\ldots, x_n]$, and $y$ is the output vector $[y_1, \\ldots, y_m]$.  We see thus that at the heart of a layer of ReLU units is a matrix multiplication, followed by a max operator. \n", "\n", "The matrix multiplication is perfomed by a PyTorch [linear layer](https://pytorch.org/docs/stable/nn.html#linear-layers), and the maximum operator via the [Pytorch relu function](https://pytorch.org/docs/stable/nn.functional.html#relu).   "]}, {"cell_type": "markdown", "metadata": {"id": "420D3Xj-aC8G"}, "source": ["The last layer of our neural net is a _softmax_ layer, as is common in classification problems.  Indicate by $y_0, \\ldots, y_9$ be the 10 outputs of the 10 linear neurons, computed via $b + Wx$.  These $y_0, \\ldots, y_9$ are general real numbers, not restricted to a particular range.  As our aim is to identify which one among the 10 digits is the digit in the input, we prefer the output of the overall layer to be a vector $[p_0, \\ldots, p_9]$ of probabilities for each digit $0, \\ldots, 9$, with $0 \\leq p_i \\leq 1$ for all $0 \\leq i \\leq 9$, and $\\sum_{i=0}^9 p_i = 1$.  We convert from linear outputs to probabilities using the [_softmax_](https://en.wikipedia.org/wiki/Softmax_function) transformation: \n", "\n", "$$\n", "p_i = \\frac{e^{y_i}}{\\sum_{j=0}^9 e^{y_j}} \\; . \n", "$$\n", "\n", "It turns out that is numerically more stable to output not these probabilities directly, but their logarithm.  We output:\n", "\n", "$$\n", "q_i = \\log p_i = (\\log e^{y_i}) - \\log\\sum_{j=0}^9 e^{y_j} = \n", "y_i - \\log\\sum_{j=0}^9 e^{y_j} \\; . \n", "$$"]}, {"cell_type": "markdown", "metadata": {"id": "Y6abEjcAZyrN"}, "source": ["The definition of our net is done in the class `FlatNet` below.  We define the three linear layers in the `__init__` method: these are the layers that contain trainable weights.  The computation of the output of the net happens in the `forward` method.  The input `x` is a tensor, that is, a matrix with size $[28 * 28, \\kappa]$, where $\\kappa$ is the batch size: the net will process the inputs for $\\kappa$ digits at a time.  To compute the net output, we apply the transformation for each layer. "]}, {"cell_type": "code", "metadata": {"id": "CT3h_Zs4xqMb"}, "source": ["class FlatNet(nn.Module):\n", "    \"\"\"This is a 3-layer NN, flat (not convolutional).\"\"\"\n", "\n", "    def __init__(self, layer1_size=128, layer2_size=128):\n", "        super().__init__()\n", "        self.layer1 = nn.Linear(28 * 28, layer1_size)\n", "        self.layer2 = nn.Linear(layer1_size, layer2_size)\n", "        self.layer3 = nn.Linear(layer2_size, 10)\n", "\n", "    def forward(self, x):\n", "        \"\"\"x is the input to the NN.  Computes the output.\"\"\"\n", "        x = F.relu(self.layer1(x))\n", "        x = F.relu(self.layer2(x))\n", "        return F.log_softmax(self.layer3(x), dim=1)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "JTJLicrF0sWD"}, "source": ["### Training and testing\n", "\n", "#### Training and testing data\n", "\n", "Once the network is defined, we can write two functions, for training and testing the network.  The two functions are very similar, except that they work on different portions of the data, and that only the training function actually trains the network. \n", "\n", "Separating the dataset into a trainign and a testing portion may seem wasteful: why not train from all the data, rather than from only a portion of it?  The reason for using separate training and testing data is that we want to be able to check for _overtraining_.  If a neural network has many degrees of freedom (many trainable weights) compared to how much training data we have, it is possible that the network will learn the particular examples on which we train it, rather than understand the general rules that makes a \"2\" a \"2\" and a \"7\" a \"7\" digit.  We want the network to understand the rules, not to memorize the training examples: our goal is to use the network to classify _new_ data that it has not seen yet, new digits written by people in the future, not the same old digits we have in the training set. \n", "\n", "To check whether the network is overtraining, we will measure the network classification accuracy on testing data that has not been used to train the network.  We split the overall data (actually, MNIST comes pre-split) in two sets: a training set consisting of 80% of the data, and a testing set consisting of 20% of the data. "]}, {"cell_type": "markdown", "metadata": {"id": "NQQhV7OJoCwO"}, "source": ["#### The model and optimizer\n", "\n", "The `model` is an instance of our `FlatNet` net.  \n", "The `optimizer` is in charge of optimizing the model: precisely, the optimizer decides the size of the learning step, and it applies the update to all the model parameters.   The optimizer is created by giving its initializer the parameters of the model, and a learning rate (a base learn step size). There are [many optimizers available in PyTorch](https://pytorch.org/docs/stable/optim.html), and the [SGD optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) is the simplest of them all.  It implements standard gradient descent, with a bit of \"[momentum](http://www.cs.toronto.edu/~hinton/absps/momentum.pdf)\".  A more sophisticated optimizer, such as [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam), could be used as a drop-in replacement for the SGD one; you are encouraged to experiment with this.  Note that when you change the optimizer, you may have to give a different base learning rate, as each optimizer may interpret this rate in a different way. "]}, {"cell_type": "code", "metadata": {"id": "qVGdGU5T1dy-"}, "source": ["model = FlatNet(layer1_size=128, layer2_size=128).to(device)\n", "\n", "LEARNING_RATE = 0.01\n", "MOMENTUM = 0.5\n", "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "oVA6XMBHi7od"}, "source": ["#### The `train` and `test` functions\n", "\n", "The `train` and `test` functions correspond closely to the code we used to train our homebrew models.  The `train` function iterates over the training data, one batch at a time.  The `data.to(device)` and `target.to(device)` expressions move the data (the images to be classified) and the target (the correct digits) to the `device`, which is a GPU (if you wish to run this on the CPU rather than the GPU, change `USE_CUDA` to False).\n", "\n", "First, we zero the gradient, to start the computation of a new gradient, exactly as we did in our homegrown framework. \n", "\n", "Next, the images in the batch are flattened, that is, transformed from a square grid of pixels to a linear vector of $28 * 28$ pixels, and they are fed to the model, which is an instance of our `FlatNet` class.  \n", "\n", "We then compute the loss via the [_log-likelyhood_](https://en.wikipedia.org/wiki/Likelihood_function) function.  The details of why we use this loss are not essential; suffices to say, the log likelyhood measures the difference between our output digit proability distribution, and the true distribution, which is a distribution with a 1 for the correct digit and 0 for all other digits.\n", "\n", "Finally, the loss gradient is propagated, via the `backward()` method call, and the parameters are updated, in the `optimizer.step()` method call.  "]}, {"cell_type": "code", "metadata": {"id": "3TMoRI6u0fAs"}, "source": ["def train(model, device, train_loader, optimizer, epoch, batch_size,\n", "          flatten=False, log_interval=100):\n", "    model.train()\n", "    correct = 0.\n", "    for batch_idx, (data, target) in enumerate(train_loader):\n", "        data, target = data.to(device), target.to(device)\n", "        optimizer.zero_grad()\n", "        output = model(data.view(-1, 28 * 28)) if flatten else model(data)\n", "        loss = F.nll_loss(output, target)\n", "        loss.backward()\n", "        optimizer.step()\n", "        # Get the prediction.\n", "        pred = output.max(1, keepdim=True)[1]\n", "        correct += pred.eq(target.view_as(pred)).sum().item()\n", "        if (batch_idx + 1) % log_interval == 0:\n", "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tAccuracy:{:.5f}'.format(\n", "                epoch, batch_idx * len(data), len(train_loader.dataset),\n", "                100. * batch_idx / len(train_loader), loss.item(),\n", "                100. * correct / (log_interval * batch_size)))\n", "            correct = 0.\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "8XwqbGHC0zi1"}, "source": ["def test(model, device, test_loader, flatten=False):\n", "    model.eval()\n", "    test_loss = 0\n", "    correct = 0.\n", "    with torch.no_grad():\n", "        for data, target in test_loader:\n", "            data, target = data.to(device), target.to(device)\n", "            output = model(data.view(-1, 28 * 28)) if flatten else model(data)\n", "            test_loss += F.nll_loss(output, target).item() # sum up batch loss\n", "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n", "            correct += pred.eq(target.view_as(pred)).sum().item()\n", "\n", "    test_loss /= len(test_loader.dataset)\n", "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.5f}%)\\n'.format(\n", "        100. * test_loss, correct, len(test_loader.dataset),\n", "        100. * correct / len(test_loader.dataset)))\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "rLJP7B9Z09gL"}, "source": ["### Let's give it a spin\n", "\n", "We are finally ready to give our neural network a spin.  \n", "\n", "#### Epochs\n", "\n", "The training occurs in _epochs_, where an epoch, in machine-learning parlance, consists in feeding to the network the entire training data.  The data is obtained via training and testing loaders: these loaders shuffle the original data so that it is presented in a different order in each epoch, and then return the data in batches of a chosen size. \n", "\n", "Shuffling the data at each epoch helps the network train uniformly over the whole dataset.\n", "\n", "#### Learning in batches\n", "\n", "We do not feed the learning examples (the digits) to our neural network one by one: rather, we feed them to the network in batches.  \n", "\n", "One reason for this is efficiency.  The computations in a batch are performed in a GPU, and a GPU is able to perform many operations in parallel.  If we fed the data one by one, we would be able to exploit the parallel processing capabilities of a GPU only in part, as the size of a single example would limit the amount of available parallelism.  For instance, in a digit, we would be limited to performing $28 \\cdot 28 \\cdot l_1$ operations in parallel, where $l_1$ is the number of layers in the first network.  If we batch $\\kappa$ examples together, this number grows by a factor of $\\kappa$. \n", "\n", "The other reason, however, is more fundamental, and it has to do with the very process for machine learning.  Ideally, we would like to tune the neural network so as to decrease its loss (its error) not on one example only, but on all training data at once: after all, we want it to recognize all digits.   Denoting by $L_x$ the loss for a particular input $x$, we would like to compute the loss $L_T = \\sum_{x \\in T} L_x$ over the whole training set $T$.  We would then train the model parameters (the weights) $\\theta_1, \\ldots, \\theta_m$ according to the complete gradient \n", "\n", "$$\n", "\\nabla_\\theta L_T = \\left( \n", "\\frac{\\partial L_T}{\\partial \\theta_1}, \\ldots, \\frac{\\partial L}{\\partial \\theta_m} \\; .\n", "\\right)\n", "$$\n", "\n", "To do this, we can call the `zero_gradient()` method at the beginning of a training epoch.  Then, we propagate each training batch through the network, and backpropagate the loss; this accummulates the gradient, and allows the computation of $g_T = \\nabla_\\theta L_T$. \n", "\n", "However, it turns out it is often better to compute the gradient for each batch, and apply it, rather than add the batch gradients into the total gradient.  That is, if $L_B = \\sum_{x \\in B} L_x$ is the sum for a batch of training data $B$, after processing $B$, we train the network with respect to the batch gradient $g_B = \\nabla_\\theta L_B$ only. The batch gradient $g_B$ is an approximation of the full gradient $g_T$, in the same way in which the average height of 10 students in a class provides an approximation for the average height of all students in the class.  Saying this in an equivalent way, we can write $g_B = g_T + \\eta$, where $\\eta$ is noise.  The larger the batch size, the smaller the noise $\\eta$ that enters the gradient at each training step. \n", "\n", "The process of using the gradient, estimated from a subset of the data, and thus affected by noise, is known as [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).  This is an accurate yet magnifically grand-sounding name for the simple process of estimating the gradient, and then following the estimate.  The next time your read only part of the assigned reading, you can tell the professor you are doing stochastic learning.\n", "\n", "It turns out that a small amount of training noise is actually beneficial, for two reasons.  First, some amount of noise is often helpful in learning, as it can be used to break symmetries, avoid getting stuck in small local minima, and similar.  Second, even the complete gradient $g_T$ is an approximation for the true (and uncomputable) gradient $g_U$ computed on the universe $U$ of possible input data.  Training on a noisy version of $g_T$ helps avoid overtraining to $T$. \n", "\n", "In practice, as for many things in machine learning, the choice of batch size is guided by experimentation.  We use here a batch size of 100; in a 100-input batch, each digit will be represented 10 times on average, so this is a good compromise between an example-specific gradient, and the gradient on all training data.\n", "\n", "#### Measuring the performance\n", "\n", "After each epoch of training, we feed to the classifier the testing data, and we print the resulting accuracy."]}, {"cell_type": "code", "metadata": {"id": "DQ__DTTj1ZHF"}, "source": ["# How big are the batches for training and testing.\n", "TRAIN_BATCH_SIZE = 100\n", "TEST_BATCH_SIZE = 100\n", "\n", "# Loads the datasets.\n", "train_loader = torch.utils.data.DataLoader(\n", "    datasets.MNIST('../data', train=True, download=True,\n", "                    transform=transforms.ToTensor()),\n", "    batch_size=TRAIN_BATCH_SIZE, shuffle=True, **kwargs)\n", "test_loader = torch.utils.data.DataLoader(\n", "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n", "    batch_size=TEST_BATCH_SIZE, shuffle=True, **kwargs)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "MPkc0b_R42yw"}, "source": ["NUM_EPOCHS = 2\n", "torch.manual_seed(1) # Init random number generator.\n", "for epoch in range(1, NUM_EPOCHS + 1):\n", "    train(model, device, train_loader, optimizer, epoch, TRAIN_BATCH_SIZE, flatten=True)\n", "    test(model, device, test_loader, flatten=True)\n"], "execution_count": null, "outputs": [{"output_type": "stream", "text": ["Train Epoch: 1 [9900/60000 (16%)]\tLoss: 2.207411 \tAccuracy:18.96000\n", "Train Epoch: 1 [19900/60000 (33%)]\tLoss: 1.902946 \tAccuracy:58.52000\n", "Train Epoch: 1 [29900/60000 (50%)]\tLoss: 1.388401 \tAccuracy:70.07000\n", "Train Epoch: 1 [39900/60000 (66%)]\tLoss: 0.916254 \tAccuracy:77.21000\n", "Train Epoch: 1 [49900/60000 (83%)]\tLoss: 0.630507 \tAccuracy:81.97000\n", "Train Epoch: 1 [59900/60000 (100%)]\tLoss: 0.436533 \tAccuracy:84.18000\n", "\n", "Test set: Average loss: 0.5567, Accuracy: 8513.0/10000 (85.13000%)\n", "\n", "Train Epoch: 2 [9900/60000 (16%)]\tLoss: 0.657593 \tAccuracy:85.41000\n", "Train Epoch: 2 [19900/60000 (33%)]\tLoss: 0.486896 \tAccuracy:86.57000\n", "Train Epoch: 2 [29900/60000 (50%)]\tLoss: 0.470603 \tAccuracy:87.30000\n", "Train Epoch: 2 [39900/60000 (66%)]\tLoss: 0.432718 \tAccuracy:87.87000\n", "Train Epoch: 2 [49900/60000 (83%)]\tLoss: 0.669593 \tAccuracy:88.45000\n", "Train Epoch: 2 [59900/60000 (100%)]\tLoss: 0.200170 \tAccuracy:89.24000\n", "\n", "Test set: Average loss: 0.3719, Accuracy: 8959.0/10000 (89.59000%)\n", "\n"], "name": "stdout"}]}, {"cell_type": "markdown", "metadata": {"id": "chVRC51j5UtD"}, "source": ["## Convolutional neural network models\n", "\n", "Once we have created our first neural network, it is a fairly simple matter to replace it with a more sophisticated one.  We will see how to replace our \"flat\" neural network with a [_convolutional_](https://en.wikipedia.org/wiki/Convolutional_neural_network) one.\n", "\n", "In our flat neural network, the $28 \\times 28$ image is flattened into an array of $28 \\cdot 28 = 784$ pixels, and these 784 pixels are then fed to the neural network.  Thus, all spacial information in the image -- the information on which pixel is close to which other pixel -- is lost.  The network will learn the weights for pixels 23, 457, 537, and so on, but will have no idea of where they are in the image!  Clearly, the network would be implementing a very different method of image processing than the one we perform in our brains! \n", "\n", "[Convolutional networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) do a much better job of exploiting spacial information.  They examine the image through a sliding window of a given size, in our case, $3 \\times 3$ pixels, and compute intermediate results on the basis of such sliding window.  Sliding a $3 \\times 3$ window over a $28 \\times 28$ image with padding of 1 again generates a $28 \\times 28$ grid of results; you can [find here](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) a good visualization of how convolution works, and what padding does.  The original pixels of the $28 \\times 28$ image had one value per pixel, the brightness value.  The resulting $28 \\times 28$ grid of results has more than one value per cell: it now has as many values as there are neurons in the $3 \\times 3$ convolution.  We use multiple neurons so that each of the neurons can learn to look for a specific aspect in the $3 \\times 3$ grid: one neuron might learn to look for vertical lines, one for horizonal lines, and so forth.  A good discussion with examples is provided in [Figure 3 of this well-kwown paper](https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf).\n", "\n", "After the convolutional step, we _pool_ the results, computing the maximum in every $2 \\times 2$ square, and reducing the image to a $14 \\times 14$ image, as a consequence.  The pooling is used to establish that features are present, while \"forgetting\" the details of their spacial placement. \n", "\n", "We repeat the convolution and pooling a second time, obtaining a $7 \\times 7$ grid of results containing 20 values per cell.  These results are then flattened into a $7 \\times 7 \\times 20$ flat array, and this flat array is passed through two flat layers of neural nets. \n", "\n", "This takes a lot of words to say, but only a few lines of code to do.  We use the [`Conv2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) layer for convolution, and the [`MaxPool2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d) layer for the pooling.  Rather than writing `__init__` and `forward` methods, we simply list our layers as input to the `nn.Sequential` model: this is a convenient shorthand for defining a model.\n"]}, {"cell_type": "code", "metadata": {"id": "Q4nJSDFSLIQx"}, "source": ["conv1_neurons = 20\n", "conv2_size = 2\n", "conv2_neurons = 40\n", "\n", "flat1_in = (28 // 4) * (28 // 4) * conv2_neurons\n", "flat1_out = 256\n", "flat2_out = 128\n", "\n", "conv_model = nn.Sequential(\n", "    nn.Conv2d(1, conv1_neurons, 3, padding=1),\n", "    nn.ReLU(),\n", "    nn.MaxPool2d(2),\n", "    nn.Conv2d(conv1_neurons, conv2_neurons, 3, padding=1),\n", "    nn.ReLU(),\n", "    nn.MaxPool2d(2),\n", "    nn.Flatten(),\n", "    nn.Linear(flat1_in, flat1_out),\n", "    nn.ReLU(),\n", "    nn.Linear(flat1_out, flat2_out),\n", "    nn.ReLU(),\n", "    nn.Linear(flat2_out, 10),\n", "    nn.LogSoftmax(dim=1)\n", ")\n", "gpu_conv_model = conv_model.to(device)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "HWtam-_6TVxO"}, "source": ["Each optimizer is defined on the basis of the model parameters, so when we change the model, we need to define a new optimizer too -- it would not work to use the optimizer for the previous linear model to train this convolutional model! "]}, {"cell_type": "code", "metadata": {"id": "VpA5UbCdSxjW"}, "source": ["conv_optimizer = optim.SGD(gpu_conv_model.parameters(), lr=0.01, momentum=0.5)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "BrQzx-zFT6Sv"}, "source": ["We can now train our convolutional model.  This is not a small net, and it will train much faster on a GPU-enabled machine (if you are on Google Colab, you can select a non-GPU kernel and a GPU kernel to see the difference for yourself)."]}, {"cell_type": "code", "metadata": {"id": "1i6M7UkmOxV8"}, "source": ["for epoch in range(1, NUM_EPOCHS + 1):\n", "    train(gpu_conv_model, device, train_loader, conv_optimizer, epoch, TRAIN_BATCH_SIZE)\n", "    test(gpu_conv_model, device, test_loader)\n"], "execution_count": null, "outputs": [{"output_type": "stream", "text": ["Train Epoch: 1 [9900/60000 (16%)]\tLoss: 2.295057 \tAccuracy:10.40000\n", "Train Epoch: 1 [19900/60000 (33%)]\tLoss: 2.282201 \tAccuracy:14.07000\n", "Train Epoch: 1 [29900/60000 (50%)]\tLoss: 2.240697 \tAccuracy:23.02000\n", "Train Epoch: 1 [39900/60000 (66%)]\tLoss: 1.894892 \tAccuracy:52.45000\n", "Train Epoch: 1 [49900/60000 (83%)]\tLoss: 0.660757 \tAccuracy:71.90000\n", "Train Epoch: 1 [59900/60000 (100%)]\tLoss: 0.555014 \tAccuracy:82.92000\n", "\n", "Test set: Average loss: 0.4584, Accuracy: 8628.0/10000 (86.28000%)\n", "\n", "Train Epoch: 2 [9900/60000 (16%)]\tLoss: 0.399961 \tAccuracy:86.14000\n", "Train Epoch: 2 [19900/60000 (33%)]\tLoss: 0.300900 \tAccuracy:87.70000\n", "Train Epoch: 2 [29900/60000 (50%)]\tLoss: 0.293234 \tAccuracy:89.88000\n", "Train Epoch: 2 [39900/60000 (66%)]\tLoss: 0.358776 \tAccuracy:89.94000\n", "Train Epoch: 2 [49900/60000 (83%)]\tLoss: 0.376492 \tAccuracy:90.63000\n", "Train Epoch: 2 [59900/60000 (100%)]\tLoss: 0.175339 \tAccuracy:91.05000\n", "\n", "Test set: Average loss: 0.2457, Accuracy: 9237.0/10000 (92.37000%)\n", "\n"], "name": "stdout"}]}, {"cell_type": "markdown", "metadata": {"id": "gnNL0ZG43pdT"}, "source": ["## Defining new layers\n", "\n", "Most ML tutorials at this point go on to more sophisticated neural architectures or more challenging problems; [a number of very good tutorials](https://pytorch.org/tutorials/) is available on the PyTorch site, and more are available if you search.  We will do something different here: we go back to basics, to the mechanism that makes ML work.  In the previous chapter, we have seen how to implement automatic gradient computation for simple expressions; it is now time to see how to do the same in PyTorch: this will make the relationship between our simple autogradient setup and PyTorch clear. \n", "\n", "The task we will set ourselves to do is to define a new layer, which we call the `SinLayer` layer.  The `SinLayer` layer takes as input a vector $x = [x_1, \\ldots, x_n]$, and internally has a set of weights $[w_1, \\ldots, w_n]$.  The output of the layer is \n", "\n", "$$\n", "    \\sin(wx) = [\\sin(w_1 x_1), \\ldots, \\sin(w_n, x_n)] \\; .\n", "$$\n", "\n", "There is no particular reason to think this layer should work especially well.  Indeed, we can think of many reasons why it should work poorly, including the fact that normally, the activation functions used in machine learning have uniform behavior when the input goes to infinity, not an oscillatory behavior.  But we will give it a try nevertheless: the goal is to learn implementing a new layer, rather than obtaining the ultimate neural network for MNIST. "]}, {"cell_type": "markdown", "metadata": {"id": "EndnQgIYrtv4"}, "source": ["### Defining an autogradient function\n", "\n", "At the core of the `SinLayer` layer is the $\\sin(wx)$ function.  We could simply implement the function using the $*$ and `torch.sin` functions of PyTorch, but rather than doing that, we will instead define a custom function called `TunedSin`: in this way, we will get a close look at how the automatic gradient computation works in PyTorch.  \n", "\n", "To implement `TunedSin`, we need to implement a subclass of [`torch.autograd.function.Function`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function).  The class has two static methods: a `forward` method, which computes $\\sin(wx)$ from $x$ and $w$, and a `backward` method, that propagates the gradient. \n", "\n", "We wrote the `forward` method already.  The method takes three arguments: a context `ctx`, and $x$ and $w$.  The context is a place we can use to store values that we wish to have available during the computation of the gradient.  We store in the context $x$, $w$, and also their product $xw$; of course, the product can be reconstructed from $x$ and $w$, but if we need it, there is no point computing it both in the forward and in the backward pass.  By storing it, we are trading a little bit of memory in excange of a little bit of efficiency.  Aside from storing $x, w$, and $xw$ in the context, the forward method simply returns $\\sin(wx)$. \n", "\n", "We leave the interesting bits of the `backward` method for you to write.  The method receives as input the context, and the gradient of the loss with respect to its output, that is, indicating with $L$ the loss and with $y = \\sin(wx)$ the output of the forward pass, it receives \n", "\n", "$$\n", "    \\mathtt{grad\\_output} = \\frac{\\partial L}{\\partial y} \\; .\n", "$$\n", "\n", "Corresponding to the two inputs $x$ and $w$, the `backward` method must return the two gradients $\\partial L / \\partial x$ and $\\partial L / \\partial w$.  These gradients are computed using:\n", "\n", "$$\n", "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = \\frac{\\partial L}{\\partial y} \\frac{\\partial}{\\partial x} \\sin(wx) = \\frac{\\partial L}{\\partial y} w\\cos(wx) \\; ,\n", "$$\n", "\n", "and similarly, \n", "\n", "$$\n", "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} x\\cos(wx) \\; . \n", "$$\n", "\n", "The `backward` method first retrieves the values of $x, w$, and $xw$ from the context.  Then, the method must compute and return $\\partial L / \\partial x$ and $\\partial L / \\partial w$ as above, and return them.  We leave it to you to write this interesting, if very brief, bit of code."]}, {"cell_type": "code", "metadata": {"id": "QQ51Pun2A72O"}, "source": ["### Implementation of the `TunedSin` function\n", "\n", "from torch.autograd.function import Function\n", "\n", "class TunedSin(Function):\n", "    \"\"\"Implements sin(w * x), where w is a tunable weight vector.\"\"\"\n", "\n", "    @staticmethod\n", "    def forward(ctx, x, w):\n", "        \"\"\"\n", "        @param ctx: context, we can use to store x in it.\n", "        @param x: the tensor to propagate; it has shape b * n, where b is the\n", "            batch size, and n is the number of inputs.\n", "        @param w: weight vector, of the same shape as x.\n", "        @returns: sin(w * x)\n", "        \"\"\"\n", "        # We form the element-by-element product of x and w.\n", "        xw = x * w\n", "        # We save, in case it is useful for backward propagation,\n", "        # x, w, and the product xw.\n", "        ctx.save_for_backward(x, w, xw)\n", "        # We compute the result.\n", "        return torch.sin(xw)\n", "\n", "    @staticmethod\n", "    def backward(ctx, grad_output):\n", "        \"\"\"\n", "        @param ctx: the context, from which we can retrieve x and w.\n", "        @param grad_output: the gradient of the output.\n", "        @returns a pair (gx, gw), consisting of the gradient with\n", "            respect to x, and the gradient with respect to w.\n", "        \"\"\"\n", "        # We retrieve x, w, xw from the context.\n", "        x, w, xw = ctx.saved_tensors\n", "        # Now we must compute gx and gw.\n", "        ### BEGIN SOLUTION\n", "        raise NotImplementedError()\n", "        ### END SOLUTION\n", "        return gx, gw\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "pqZFZBLPKIBz"}, "source": ["Let us test this function in the small, so we can check that we have implemented it correctly before moving on.  Let us load our classical test library."]}, {"cell_type": "code", "metadata": {"id": "ZNKJhP6RRk6P", "cellView": "form"}, "source": ["#@title Importing the `nose` tools\n", "\n", "try:\n", "    from nose.tools import assert_equal, assert_almost_equal\n", "    from nose.tools import assert_true, assert_false\n", "    from nose.tools import assert_not_equal\n", "except:\n", "    !pip install nose\n", "    from nose.tools import assert_equal, assert_almost_equal\n", "    from nose.tools import assert_true, assert_false\n", "    from nose.tools import assert_not_equal\n"], "execution_count": null, "outputs": [{"output_type": "stream", "text": ["Collecting nose\n", "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n", "\r\u001b[K     |\u2588\u2588\u258f                             | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u258e                           | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u258d                         | 30kB 3.0MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                       | 40kB 1.9MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                     | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                   | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                 | 71kB 3.2MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588             | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f          | 102kB 3.1MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e        | 112kB 3.1MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d      | 122kB 3.1MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c    | 133kB 3.1MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 143kB 3.1MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 153kB 3.1MB/s eta 0:00:01\r\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 3.1MB/s \n", "\u001b[?25hInstalling collected packages: nose\n", "Successfully installed nose-1.3.7\n"], "name": "stdout"}]}, {"cell_type": "markdown", "metadata": {"id": "qHwObbecdaAN"}, "source": ["Let us define a shorthand function to compare sensors. "]}, {"cell_type": "code", "metadata": {"id": "s30xHP3pSogM"}, "source": ["# Let's define a function to compare tensors.\n", "def assert_tensors_equal(x, y, places=3):\n", "    v = torch.sum(torch.abs(x - y)).item()\n", "    assert_almost_equal(v, 0., places=places)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "X1dB0WhHdeLs"}, "source": ["The forward and backward method take a _context_ as their first argument.  Rather than producing a proper context, we create a _mock_ of a context.  In computer science, a _mock_ is a stand-in for the real thing, that is used in testing.  For instance, a database mock is something that (for small amounts of test data) behaves like a database, but is simpler to create, and is suited to be used in tests.  Mocks are instrumental in testing complex code, and are easier to create in \"duck typing\" languages such as Python, where you just need to create an object with the required behavior and methods. "]}, {"cell_type": "code", "metadata": {"id": "I_OQXMpuW6Ep"}, "source": ["# We also need to define a context mock for our tests\n", "class MockContext(object):\n", "\n", "    def __init__(self):\n", "        self.saved_tensors = None\n", "\n", "    def save_for_backward(self, *tensors):\n", "        self.saved_tensors = tensors\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "j_mygnpbeJc9"}, "source": ["With this, we can finally test our `TunedSin` function."]}, {"cell_type": "code", "metadata": {"id": "cWF0WfJMKOcu"}, "source": ["### Tests for `TunedSin`\n", "\n", "# Let's build two tensors x and w of the same size.\n", "x = torch.tensor([1., 2.])\n", "w = torch.tensor([2., 3.])\n", "\n", "# Let's test the forward method (this should work!).\n", "ctx = MockContext()\n", "y = TunedSin.forward(ctx, x, w)\n", "yy = torch.tensor([ 0.9093, -0.2794])\n", "assert_tensors_equal(y, yy)\n", "\n", "# And the backpropagation.\n", "grad_output = torch.tensor([1.2, 1.6])\n", "gx, gw = TunedSin.backward(ctx, grad_output)\n", "assert_tensors_equal(gx, torch.tensor([-0.9988,  4.6088]))\n", "assert_tensors_equal(gw, torch.tensor([-0.4994,  3.0725]))\n", "\n", "# Once more.\n", "x = torch.tensor([-1., 0.1])\n", "w = torch.tensor([0.2, 2.1])\n", "y = TunedSin.forward(ctx, x, w)\n", "grad_output = torch.tensor([-1., 3.4])\n", "gx, gw = TunedSin.backward(ctx, grad_output)\n", "assert_tensors_equal(gx, torch.tensor([-0.1960,  6.9831]))\n", "assert_tensors_equal(gw, torch.tensor([0.9801, 0.3325]))\n", "\n", "# Oh, let's check that when we backpropagate 0 we get 0.\n", "gx, gw = TunedSin.backward(ctx, torch.tensor([0., 0.]))\n", "assert_tensors_equal(gx, torch.tensor([0., 0.]))\n", "assert_tensors_equal(gw, torch.tensor([0., 0.]))\n", "\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "6U5jq6_EeQIy"}, "source": ["### Defining the `SinLayer` layer\n", "\n", "We are now ready to define our new layer.  Layers in PyTorch are called modules, and are subclasses of the `nn.Module` class.  Models usually have learnable parameters, which are declared as subclasses of the [`Parameter` class](https://pytorch.org/docs/stable/nn.html#parameters).  When we will build an optimizer later on, we will call the [`parameter()` method](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.parameters) on the overall module; that method call will collect all instances of `Parameter` from all the sub-models of the model (all the layers of the network), and return it as the set of parameters to be optimized. "]}, {"cell_type": "code", "metadata": {"id": "Klxq223HXO8a"}, "source": ["from torch.nn.parameter import Parameter\n", "\n", "class SinLayer(nn.Module):\n", "\n", "    def __init__(self, in_features):\n", "        \"\"\"Initializes the layer, which is done to process in_features.\"\"\"\n", "        super().__init__() # Let's not forget to initialize the Module.\n", "        self.in_features = in_features\n", "        # We would expect a size (in_features,) rather than (1, in_features),\n", "        # but the first 1, is for the batch size.  In this way, when we pass\n", "        # to the layer a batch of size (b, in_features), the weights self.w\n", "        # will be \"broadcast\" from shape (1, in_features) to shape\n", "        # (b, in_features).  We fill the weights with zeros now, but we\n", "        # then overwrite them with random values.\n", "        self.w = Parameter(torch.zeros(1, in_features))\n", "        # Let's overwrite it with random values.  In a tensor, self.w.data is\n", "        # the actual data (as a numpy array).\n", "        self.w.data.normal_()\n", "\n", "    def forward(self, x):\n", "        \"\"\"Propagates the (batch of) values x.\"\"\"\n", "        # We just apply our TunedSin function.\n", "        return TunedSin.apply(x, self.w)\n", "\n", "    def extra_repr(self):\n", "        return \"in_features=out_features=%d\" % self.in_features\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "BJfInozIgtvE"}, "source": ["We can now define the overall model.  We will define it using the `nn.Sequential` shorthand, as we did for the convolutional networks.  You need to build a sequence of layers like this: \n", "\n", "* A `Linear` layer, with as input as many pixels as a MNIST image, and as output, $n$ outputs. \n", "* A `SinLayer` layer, with $n$ inputs and outputs. \n", "* A `Linear` layer, with $n$ inputs and ouputs. \n", "* A `SinLayer` layer, with $n$ inputs and outputs. \n", "* A `Linear` layer, with $n$ inputs and 10 outputs.\n", "* A final `LogSoftmax` layer; this is given for you. \n", "\n", "We will test later that the model is correctly built.  We will have you experiment with $n = 64, 128, 256, 512$.  We call $n$ the `LAYER_SIZE` in the code."]}, {"cell_type": "code", "metadata": {"id": "6bHMZZ_zhwu3"}, "source": ["#@title Choose your layer size\n", "LAYER_SIZE = 64 #@param {type:\"integer\"}\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "lu8lZ85radoT"}, "source": ["### Creating a `sin_model`\n", "\n", "sin_model = nn.Sequential(\n", "    ### BEGIN SOLUTION\n", "    raise NotImplementedError()\n", "    ### END SOLUTION\n", "    nn.LogSoftmax(dim=1)\n", ").to(device)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Dw2xf9bbc-n0"}, "source": ["Let us check that you built the model correctly.  If this check does not pass, there is no point in your trying to train the model. "]}, {"cell_type": "code", "metadata": {"id": "EwYw7xsebrfi"}, "source": ["### Tests for `sin_model`\n", "\n", "# Let us check that you built the model correctly.\n", "correct_fmt = [\n", "    \"Linear(in_features=784, out_features={sz}, bias=True)\",\n", "    \"SinLayer(in_features=out_features={sz})\",\n", "    \"Linear(in_features={sz}, out_features={sz}, bias=True)\",\n", "    \"SinLayer(in_features=out_features={sz})\",\n", "    \"Linear(in_features={sz}, out_features=10, bias=True)\",\n", "    \"LogSoftmax()\"]\n", "correct_layer_types = [l.format(sz=LAYER_SIZE) for l in correct_fmt]\n", "print(\"Correct layer types:\")\n", "for c in correct_layer_types:\n", "    print(\"\\t\", c)\n", "print(\"Actual layer types:\")\n", "for c in sin_model.children():\n", "    print(\"\\t\", c)\n", "declared_layer_types = [repr(layer) for layer in sin_model.children()]\n", "assert correct_layer_types == declared_layer_types, \"Wrong layer types\"\n"], "execution_count": null, "outputs": [{"output_type": "stream", "text": ["Correct layer types:\n", "\t Linear(in_features=784, out_features=64, bias=True)\n", "\t SinLayer(in_features=out_features=64)\n", "\t Linear(in_features=64, out_features=64, bias=True)\n", "\t SinLayer(in_features=out_features=64)\n", "\t Linear(in_features=64, out_features=10, bias=True)\n", "\t LogSoftmax()\n", "Actual layer types:\n", "\t Linear(in_features=784, out_features=64, bias=True)\n", "\t SinLayer(in_features=out_features=64)\n", "\t Linear(in_features=64, out_features=64, bias=True)\n", "\t SinLayer(in_features=out_features=64)\n", "\t Linear(in_features=64, out_features=10, bias=True)\n", "\t LogSoftmax()\n"], "name": "stdout"}]}, {"cell_type": "markdown", "metadata": {"id": "NlSuX5IO5Cie"}, "source": ["You need to define now an optimizer for your model."]}, {"cell_type": "code", "metadata": {"id": "B7Jaa40Od2qZ"}, "source": ["### Defining an optimizer for the `sin_model`\n", "\n", "# Now define an optimizer.\n", "sin_optimizer = None # You need to define sin_optimizer in the next line.\n", "# You can define any optimizer you want (any algorithm you want).\n", "### BEGIN SOLUTION\n", "raise NotImplementedError()\n", "### END SOLUTION\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "GXUTZC3pd3eI"}, "source": ["# We want to make sure that this is an optimizer.\n", "assert isinstance(sin_optimizer, torch.optim.Optimizer)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "toHsVhDLaSuJ"}, "source": ["NUM_EPOCHS = 4\n", "torch.manual_seed(1) # Init random number generator.\n", "for epoch in range(1, NUM_EPOCHS + 1):\n", "    train(sin_model, device, train_loader, sin_optimizer, epoch, TRAIN_BATCH_SIZE, flatten=True)\n", "    test(sin_model, device, test_loader, flatten=True)\n"], "execution_count": null, "outputs": [{"output_type": "stream", "text": ["Train Epoch: 1 [9900/60000 (16%)]\tLoss: 1.782973 \tAccuracy:45.94000\n", "Train Epoch: 1 [19900/60000 (33%)]\tLoss: 1.233462 \tAccuracy:69.88000\n", "Train Epoch: 1 [29900/60000 (50%)]\tLoss: 0.884854 \tAccuracy:79.59000\n", "Train Epoch: 1 [39900/60000 (66%)]\tLoss: 0.642321 \tAccuracy:83.97000\n", "Train Epoch: 1 [49900/60000 (83%)]\tLoss: 0.529101 \tAccuracy:86.10000\n", "Train Epoch: 1 [59900/60000 (100%)]\tLoss: 0.490015 \tAccuracy:87.43000\n", "\n", "Test set: Average loss: 0.5088, Accuracy: 8888.0/10000 (88.88000%)\n", "\n", "Train Epoch: 2 [9900/60000 (16%)]\tLoss: 0.540992 \tAccuracy:88.69000\n", "Train Epoch: 2 [19900/60000 (33%)]\tLoss: 0.371622 \tAccuracy:89.23000\n", "Train Epoch: 2 [29900/60000 (50%)]\tLoss: 0.519902 \tAccuracy:89.81000\n", "Train Epoch: 2 [39900/60000 (66%)]\tLoss: 0.546712 \tAccuracy:89.89000\n", "Train Epoch: 2 [49900/60000 (83%)]\tLoss: 0.290569 \tAccuracy:90.42000\n", "Train Epoch: 2 [59900/60000 (100%)]\tLoss: 0.384529 \tAccuracy:90.35000\n", "\n", "Test set: Average loss: 0.3369, Accuracy: 9136.0/10000 (91.36000%)\n", "\n", "Train Epoch: 3 [9900/60000 (16%)]\tLoss: 0.278258 \tAccuracy:91.37000\n", "Train Epoch: 3 [19900/60000 (33%)]\tLoss: 0.285330 \tAccuracy:91.48000\n", "Train Epoch: 3 [29900/60000 (50%)]\tLoss: 0.224544 \tAccuracy:91.78000\n", "Train Epoch: 3 [39900/60000 (66%)]\tLoss: 0.253332 \tAccuracy:91.87000\n", "Train Epoch: 3 [49900/60000 (83%)]\tLoss: 0.272682 \tAccuracy:92.50000\n", "Train Epoch: 3 [59900/60000 (100%)]\tLoss: 0.275963 \tAccuracy:91.70000\n", "\n", "Test set: Average loss: 0.2773, Accuracy: 9269.0/10000 (92.69000%)\n", "\n", "Train Epoch: 4 [9900/60000 (16%)]\tLoss: 0.261183 \tAccuracy:92.56000\n", "Train Epoch: 4 [19900/60000 (33%)]\tLoss: 0.254691 \tAccuracy:92.70000\n", "Train Epoch: 4 [29900/60000 (50%)]\tLoss: 0.255168 \tAccuracy:92.88000\n", "Train Epoch: 4 [39900/60000 (66%)]\tLoss: 0.181025 \tAccuracy:92.95000\n", "Train Epoch: 4 [49900/60000 (83%)]\tLoss: 0.258186 \tAccuracy:93.72000\n", "Train Epoch: 4 [59900/60000 (100%)]\tLoss: 0.213238 \tAccuracy:92.92000\n", "\n", "Test set: Average loss: 0.2389, Accuracy: 9342.0/10000 (93.42000%)\n", "\n"], "name": "stdout"}]}, {"cell_type": "markdown", "metadata": {"id": "pWG7K5W57T_5"}, "source": ["Assign to BEST_LAYER_SIZE the value of LAYER_SIZE among 64, 128, 256, 512 that achieves the best performance according to the above training experiments (with the given randomization seed, trained for 4 epochs). "]}, {"cell_type": "code", "metadata": {"id": "xn1gwNms7rkG"}, "source": ["### What is the best layer size?\n", "\n", "# Assign the best layer size to LAYER_SIZE, as in (for example):\n", "# LAYER_SIZE = 128\n", "### BEGIN SOLUTION\n", "raise NotImplementedError()\n", "### END SOLUTION\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "dm6xG4vv7ziP"}, "source": ["### Tests for the best layer size\n", "\n", "assert_true(BEST_LAYER_SIZE in {64, 128, 256, 512})\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "kKiWUxJW8Io1"}, "source": ["In aeronautics, there is a saying: if you attach a powerful enough engine, you can make even a brick fly.  This is likely a good metaphor for what we did with our `SinLayer`.  Nevertheless, it is interesting to note that the `SinLayer`, on MNIST and for the brief amount of training we performed, performs a bit better than the classical ReLU neurons! "]}]}