{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"name": "Data_Structures_and_Their_Access_Characteristics_chapter.ipynb", "provenance": [], "collapsed_sections": []}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "5roLEWIe0Dbj", "colab_type": "text"}, "source": ["# Data Structures and Their Access Characteristics\n", "\n", "### \u00a9 Luca de Alfaro, 2019, [CC-BY_NC License](http://creativecommons.org/licenses/by-nc-nd/4.0/)."]}, {"cell_type": "markdown", "metadata": {"id": "2ede95ff06743130", "colab_type": "text"}, "source": ["**Note: This is not a homework. Do not submit it as an assignment solution.**\n", "\n", "This is a book chapter, and it is not formatted to be autograded. If you submit it as a homework, we will not be able to give you credit for your work."]}, {"cell_type": "markdown", "metadata": {"id": "FK2jAiqZ09mF", "colab_type": "text"}, "source": ["One of the things that makes Python so appealing for experimenting with algorithms and data science is that it comes with very flexible, and well-implemented, data structures that one can use then to build the solution to one's problem. \n", "We will review here briefly the main data structures we will be using in the course, and we will recall their approximate access characteristics. \n", "The goal is not to do a precise analysis of the running time of algorithms, but rather, to guide the choice of the data structures to be used in solving a problem.\n", "Information about the run-time of Python methods can be found on the [Python wiki](https://wiki.python.org/moin/TimeComplexity).\n"]}, {"cell_type": "markdown", "metadata": {"id": "A-PxNk9T1vUc", "colab_type": "text"}, "source": ["## Lists\n", "\n", "Lists in Python are incredibly flexible: they are at once lists (sequences of elements), arrays (lists where you can access an array at any position), stacks, and much more. \n", "\n", "A list can be created by listing its elements:"]}, {"cell_type": "code", "metadata": {"id": "BT8fiQk20AZf", "colab_type": "code"}, "source": ["my_list = [1, 2, 3, 5]"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "UTbQIofY2Lbe", "colab_type": "text"}, "source": ["The empty list is denoted by `[]`.  Lists are passed by reference, so:"]}, {"cell_type": "code", "metadata": {"id": "50LtqHil06mS", "colab_type": "code"}, "source": ["l1 = [1, 2, 3, 5]\n", "l2 = l1\n", "l1.append(7)\n", "print(l2)"], "execution_count": 0, "outputs": [{"output_type": "stream", "text": ["[1, 2, 3, 5, 7]\n"], "name": "stdout"}]}, {"cell_type": "markdown", "metadata": {"id": "dnRblZpv2bmo", "colab_type": "text"}, "source": ["There are several ways of making (shallow) copies of lists, so that a modification to one does not modify the others:"]}, {"cell_type": "code", "metadata": {"id": "RgGXaQxC2Z1q", "colab_type": "code"}, "source": ["l1 = [1, 2, 3, 5]\n", "l2 = l1[:]\n", "l3 = l1.copy()\n", "l4 = list(l1)\n", "\n", "# We modify l1, the other lists don't change.\n", "l1.append(7)\n", "print(l1, l2, l3, l4)"], "execution_count": 0, "outputs": [{"output_type": "stream", "text": ["[1, 2, 3, 5, 7] [1, 2, 3, 5] [1, 2, 3, 5] [1, 2, 3, 5]\n"], "name": "stdout"}]}, {"cell_type": "markdown", "metadata": {"id": "Mtfhu8kx243M", "colab_type": "text"}, "source": ["### List operations\n", "\n", "The list operations are described in the [Python standard library](https://docs.python.org/3/library/stdtypes.html#sequence-types-list-tuple-range), as well as in the [tutorial](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists). \n", "Their time complexity is as follows: \n", "\n", "**Accessing an element** as in `my_list[4]` takes constant time: it is _not_ necessary to traverse the list from the beginning; this is true for both reading and writing (modifying) the element.  Intuitively, the elements of a Python list (or better, the pointers to the elements) are stored as an array, so if you need to access the $k$-th element, you can compute precisely which loaction of memory to read: the location $k$ cells after the beginning of the list. There is no need to traverse the memory locations $1, 2, \\ldots, k-1$ to get to it. \n", "\n", "**Deleting an element given the index** takes time proportional to the number of elements following the deleted one, as these elements need to be rearranged. \n", "\n", "**Concatenation** of two lists has cost proportional to the length of the resulting list.\n", "\n", "**Extension** of a list has cost proportional to the length of the list being added to the existing one.\n", "\n", "**Getting the length** has constant cost, as the length is cached. "]}, {"cell_type": "markdown", "metadata": {"id": "OuLwP1WR56DP", "colab_type": "text"}, "source": ["## Dictionaries\n", "\n", "A dictionary (dict) is a mapping from keys to values.  Internally, a dictionary is implemented via a hash table.  This means, intuitively, that when accessing the element `d['hello']` for a dictionary `d`, the following happens:\n", "\n", "**1: Compute the hash.** We compute the [_hash_](https://docs.python.org/3.4/library/functions.html?highlight=file#hash) of the key, that is, we somehow compute a number out of the key, in this case the string `hello`.  Yes, this takes time proportional to the length of the key, but in practice, it's pretty fast, and if the key is too long, you can consider only some initial part of it.  For a string, we could simply add up the numbers corresponding to the characters (well, a better method is used in the actual implementation). Assume the number we generate from 'hello' is "]}, {"cell_type": "code", "metadata": {"id": "oYqLDl2X220x", "colab_type": "code"}, "source": ["hash('hello')"], "execution_count": 0, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["1029417203231997356"]}, "metadata": {"tags": []}, "execution_count": 4}]}, {"cell_type": "markdown", "metadata": {"id": "VUvEXDvB9Ovl", "colab_type": "text"}, "source": ["This dispels any belief that the hash function is computed by adding up the characters!  In fact, computing the hash is an art unto itself; some discussion can be [found in the git repository for Python](https://github.com/python/cpython/blob/bfe4fd5f2e96e72eecb5b8a0c7df0ac1689f3b7e/Python/pyhash.c).  But we don't need to worry with those things here.\n", "\n", "**2. Look in the place given by the hash.** The dictionary internally has a fixed-length list of \"cells\"; each cell can contain one or more keys.  Suppose our dictionary has 100 cells; then for the key \"hello\" we look at position:"]}, {"cell_type": "code", "metadata": {"id": "ohxPoUsJ9NXY", "colab_type": "code"}, "source": ["hash('hello') % 100"], "execution_count": 0, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["56"]}, "metadata": {"tags": []}, "execution_count": 6}]}, {"cell_type": "markdown", "metadata": {"id": "u5U7G9_K-_-u", "colab_type": "text"}, "source": ["If we find the key `'hello'` in cell 56, we are all set.  If we don't find it there, it means the key `'hello'` is not in the dictionary.  The number of cells in the dictionary is tuned so that only a few keys are in each cell. "]}, {"cell_type": "markdown", "metadata": {"id": "ydf0lN6S_bn_", "colab_type": "text"}, "source": ["### Time complexity of dictionary operations\n", "\n", "Since the hash of a key can be computed more or less in constant time (and in any case, really fast), and since each dictionary cell (on average) contains only a few keys, the upshot is that accessing one element of a dictionary via its key takes constant time, for insertion, updates, and deletions. "]}, {"cell_type": "markdown", "metadata": {"id": "Ix51MAopAJhG", "colab_type": "text"}, "source": ["## ... and the rest\n", "\n", "Lists and dictionaries are the main underlying data structures of Python.  In lists, you can compute how to access an element in position $k$.  In dictionaries, you use a hash function to figure out fast where to look for the key.\n", "\n", "Yes, there are [deques](https://docs.python.org/3.7/library/collections.html#collections.deque) and a few other specialized data structures, but by and large, you can understand the rest in terms of lists and dictionaries.  For instance, a set is just a dictionary whose keys all map to the same value (say, None).  Thus you can check set membership in constant time, and you can take the union of two sets just by adding to the first set all the keys of the second. "]}, {"cell_type": "markdown", "metadata": {"id": "_ZIM7LnDBiSz", "colab_type": "text"}, "source": ["## Numpy\n", "\n", "And then, there's [numpy](https://numpy.org/), the amazingly fast, useful, and sophisticated numerical library for Python.  Numpy has been written on the basis of an enormous amount of accumulated knowledge and experience about numerical computations, and it truly represents a peak in the computer science accomplishments. \n", "\n", "Numpy enables, among many other things, very fast operations on arrays and $n$-dimensional matrices.  \n", "Numpy is so fast, compared to Python, that I have found useful to think at the time complexity of Python programs that include numpy as follows: \n", "\n", "* In first approximation, count every numpy operation as being constant time, regardless of matrix size.  Your goal should be to minimize at all costs the number of numpy operations you perform, and _avoid at all costs_ iterating over elements of arrays (single, or multi-dimensional) directly in Python.  This often involves writing code in ways that are very different from what initially might feel natural; below we will provide some simple examples.\n", "\n", "* Only after you have minimized the number of numpy operations, should you start to worry about the matrix sizes, and the time taken by numpy itself.\n", "\n", "Let us first see for ourselves the difference in speed between numpy and Python.  We have two long arrays, and we need to compute their sum."]}, {"cell_type": "code", "metadata": {"id": "d8Z8sRQs-86F", "colab_type": "code"}, "source": ["import time\n", "import numpy as np\n", "\n", "r = 1000\n", "\n", "# First in Python\n", "a = list(range(100000))\n", "b = list(range(100000, 200000))\n", "avg_p_time = 0.\n", "for _ in range(r):\n", "    t = time.time()\n", "    # This is the iteration over elements you need so desperately to avoid.\n", "    c = []\n", "    for i in range(len(a)):\n", "        c.append(a[i] + b[i])\n", "    avg_p_time += time.time() - t\n", "print(\"Python:\", avg_p_time / r)\n", "\n", "# Then in Numpy\n", "aa = np.arange(100000)\n", "bb = np.arange(100000, 200000)\n", "avg_n_time = 0.\n", "for _ in range(r):\n", "    t = time.time()\n", "    cc = aa + bb\n", "    avg_n_time += time.time() - t\n", "print(\"Numpy:\", avg_n_time / r)\n", "print(\"Ratio:\", avg_p_time / avg_n_time)"], "execution_count": 0, "outputs": [{"output_type": "stream", "text": ["Python: 0.02266462516784668\n", "Numpy: 0.0001668970584869385\n", "Ratio: 135.80002771361268\n"], "name": "stdout"}]}, {"cell_type": "markdown", "metadata": {"id": "3xLuj_OwGwKr", "colab_type": "text"}, "source": ["About 100 times faster.  The difference in speed justifies trying to solve problems relying as much as possible on numpy's ability to operate on a whole array at once, avoiding iterating over elements.  \n", "\n", "As a simple example, consider the problem of counting how many elements in a random array are above a threshold.  The natural approach consists in keeping a counter, and iterate over the array, incrementing the counter whenever an element is over the threshold.  However, in numpy, another strategy is actually much better:\n", "\n", "* First, we compare (at once!) all the array with the threshold, obtaining a boolean array of True, False of the same length of the original array. \n", "* Then, we add (at once!) all the elements of the boolean array, obtaining the count. \n", "\n", "Let's see how the approaches compare in practice. "]}, {"cell_type": "code", "metadata": {"id": "asyHn73JF7lL", "colab_type": "code"}, "source": ["# This is our large random array.\n", "aa = np.random.random(100000)\n", "a = list(aa) # Python list\n", "r = 1000\n", "\n", "# First in Python\n", "avg_p_time = 0.\n", "for _ in range(r):\n", "    t = time.time()\n", "    c = 0\n", "    for x in a:\n", "        if x > 0.9:\n", "            c += 1\n", "    avg_p_time += time.time() - t\n", "print(\"Python:\", avg_p_time / r)\n", "\n", "# Then in numpy, still iterating.\n", "avg_n_time = 0.\n", "for _ in range(r):\n", "    t = time.time()\n", "    c = 0\n", "    for x in aa:\n", "        if x > 0.9:\n", "            c += 1\n", "    avg_n_time += time.time() - t\n", "print(\"Numpy, by iterating:\", avg_n_time / r)\n", "\n", "# Finally in numpy, first comparing, then adding.\n", "avg_nn_time = 0.\n", "for _ in range(r):\n", "    t = time.time()\n", "    c = np.sum(aa > 0.9)\n", "    avg_nn_time += time.time() - t\n", "print(\"Numpy, array ops:\", avg_nn_time / r)"], "execution_count": 0, "outputs": [{"output_type": "stream", "text": ["Python: 0.009957046270370484\n", "Numpy, by iterating: 0.0147108736038208\n", "Numpy, array ops: 0.000148221492767334\n"], "name": "stdout"}]}, {"cell_type": "markdown", "metadata": {"id": "HqaPJ3ZUJkz0", "colab_type": "text"}, "source": ["A speed difference of 100 is the speed difference between jogging (12 Km/h) and the speed of sound.  Or between walking (4.6 Km/h) and a [garden snail](https://hypertextbook.com/facts/1999/AngieYee.shtml)."]}]}